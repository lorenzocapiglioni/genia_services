#  Microservicio de Scoring de Cr茅dito con MLP y FastAPI

Este proyecto ofrece un microservicio de inferencia listo para producci贸n, empaquetado en Docker. Utiliza un modelo de Perceptr贸n Multicapa (MLP) entrenado con PyTorch para evaluar el riesgo crediticio de un solicitante en tiempo real.

##  Caracter铆sticas Principales

- API Moderna: Construido con FastAPI, que proporciona alta performance y documentaci贸n interactiva autom谩tica (Swagger UI).

- Modelo de Deep Learning: Utiliza PyTorch para las predicciones, permitiendo arquitecturas de redes neuronales complejas.

- Listo para Desplegar: Totalmente dockerizado, garantizando un entorno consistente y un despliegue sencillo.

- Preprocesamiento Integrado: El pipeline de preprocesamiento de scikit-learn est谩 integrado, asegurando que los datos de inferencia se traten igual que en el entrenamiento.

##  Gu铆a de Construcci贸n

### Paso 1: Preparaci贸n de Artefactos
- Aseg煤rate de tener los artefactos del modelo (`.pt`) y el preprocesador (`.joblib`) en la carpeta `python/credit_scoring/models/`.

### Paso 2: Construcci贸n de la Imagen Docker
- Navega al directorio ra铆z `genia_services/` y ejecuta el siguiente comando para construir la imagen.

```bash
docker build -t ingeniia/credit-scoring-mlp:1.0 -f container-images/credit_scoring/Dockerfile .
```
### Paso 3: Ejecutar el Contenedor Docker
- Una vez construida la imagen, levanta un contenedor con este comando:

```bash
 docker run -d -p 8000:8000 --name credit-scoring-service ingeniia/credit-scoring-mlp:1.0
```

### Paso 4: Verificar el Funcionamiento
- Abre tu navegador web y ve a la siguiente URL para acceder a la documentaci贸n interactiva de la API:

```bash
http://localhost:8000/docs
```

##  C贸mo Usar la API (隆Haciendo una Predicci贸n!)
El endpoint principal es /mlp_demo. Puedes enviarle una solicitud POST con los datos del solicitante en formato JSON.

- Opci贸n A: Usando la Documentaci贸n Interactiva (Swagger)

    - Ve a http://localhost:8000/docs.

    - Despliega el endpoint POST /mlp_demo.

    - Haz clic en el bot贸n "Try it out".

    - Modifica el cuerpo de la solicitud (Request body) con los datos del cliente.

    - Haz clic en "Execute". 隆Ver谩s la respuesta del modelo directamente en la p谩gina!

- Opci贸n B: Usando cURL desde la Terminal

    - Abre una terminal y ejecuta el siguiente comando cURL para enviar una solicitud de ejemplo:

        ```bash
        curl -X 'POST' \
        'http://localhost:8000/mlp_demo' \
        -H 'accept: application/json' \
        -H 'Content-Type: application/json' \
        -d '{
        "Age": 35,
        "Sex": "male",
        "Job": 2,
        "Housing": "own",
        "Saving accounts": "little",
        "Checking account": "moderate",
        "Credit amount": 2500,
        "Duration": 24,
        "Purpose": "car"
        }'
        ```

- Respuesta Exitosa Esperada (200 OK):
Si todo va bien, recibir谩s una respuesta como esta, indicando la predicci贸n (good o bad) y la probabilidad asociada:

    ```bash
    {
    "prediction": "good",
    "probability": 0.7852
    }
    ```


## 锔 Gesti贸n del Contenedor
Aqu铆 tienes algunos comandos 煤tiles para administrar el contenedor Docker.

- Puedes detener el contenedor en cualquier momento usando:

    ```bash
    docker stop credit-scoring-service  
    ```

- Ver los logs en tiempo real:
    ```bash
    docker logs -f credit-scoring-service 
    ```

- Reiniciar un contenedor detenido::
    ```bash
    docker start credit-scoring-service 
    ```

- Reiniciar un contenedor detenido::
    ```bash
    docker stop credit-scoring-service && docker rm credit-scoring-service
    ```
